{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = pd.read_csv('Datasets/reddit_cleaned_title_and_selftext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>char_count_title</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>char_count_selftext</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>title + selftext</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_selftext</th>\n",
       "      <th>clean_title_+_selftext</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothanksbud5</td>\n",
       "      <td>g5rffm</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1587516214</td>\n",
       "      <td>Wow i didn’t realize how much music is about b...</td>\n",
       "      <td>Why is almost all music seem to be about love?</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>219</td>\n",
       "      <td>36</td>\n",
       "      <td>Why is almost all music seem to be about love?...</td>\n",
       "      <td>almost music seem love</td>\n",
       "      <td>wow realize much music love romance seems like...</td>\n",
       "      <td>almost music seem love wow realize much music ...</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dontknowwhattdo</td>\n",
       "      <td>g5r7z2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1587515419</td>\n",
       "      <td>I thought that during this time it would be ni...</td>\n",
       "      <td>pieces of advice that have stuck with you?</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>285</td>\n",
       "      <td>55</td>\n",
       "      <td>pieces of advice that have stuck with you? I t...</td>\n",
       "      <td>pieces advice stuck</td>\n",
       "      <td>thought time would nice hear words encourageme...</td>\n",
       "      <td>pieces advice stuck thought time would nice he...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sharkfinnsouphk</td>\n",
       "      <td>g5r5q2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1587515173</td>\n",
       "      <td>I just can't shake this worry about kids (and ...</td>\n",
       "      <td>Worried about people stuck at home</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>269</td>\n",
       "      <td>50</td>\n",
       "      <td>Worried about people stuck at home I just can'...</td>\n",
       "      <td>worried people stuck home</td>\n",
       "      <td>shake worry kids adults stuck home lock sexual...</td>\n",
       "      <td>worried people stuck home shake worry kids adu...</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.8924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dehlen1me</td>\n",
       "      <td>g5r3t3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1587514972</td>\n",
       "      <td>https://youtu.be/9_AWrNmcMZA\\nThis is one of t...</td>\n",
       "      <td>How a 5 Dollar bill can help you to feel bette...</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>179</td>\n",
       "      <td>24</td>\n",
       "      <td>How a 5 Dollar bill can help you to feel bette...</td>\n",
       "      <td>dollar bill help feel better</td>\n",
       "      <td>https youtu awrnmcmza one amazing uplifting vi...</td>\n",
       "      <td>dollar bill help feel better https youtu awrnm...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.8555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fighterpilot909</td>\n",
       "      <td>g5qtjo</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1587513886</td>\n",
       "      <td>Imagine how insane that book would be. To make...</td>\n",
       "      <td>I want an autobiography from John McAfee so badly</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "      <td>I want an autobiography from John McAfee so ba...</td>\n",
       "      <td>want autobiography john mcafee badly</td>\n",
       "      <td>imagine insane book would make even better cou...</td>\n",
       "      <td>want autobiography john mcafee badly imagine i...</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author      id  num_comments  score  created_utc  \\\n",
       "0     nothanksbud5  g5rffm             1      2   1587516214   \n",
       "1  dontknowwhattdo  g5r7z2             3      2   1587515419   \n",
       "2  sharkfinnsouphk  g5r5q2             2      0   1587515173   \n",
       "3        dehlen1me  g5r3t3             0      1   1587514972   \n",
       "4  fighterpilot909  g5qtjo             2      2   1587513886   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Wow i didn’t realize how much music is about b...   \n",
       "1  I thought that during this time it would be ni...   \n",
       "2  I just can't shake this worry about kids (and ...   \n",
       "3  https://youtu.be/9_AWrNmcMZA\\nThis is one of t...   \n",
       "4  Imagine how insane that book would be. To make...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0     Why is almost all music seem to be about love?          0   \n",
       "1         pieces of advice that have stuck with you?          0   \n",
       "2                 Worried about people stuck at home          0   \n",
       "3  How a 5 Dollar bill can help you to feel bette...          0   \n",
       "4  I want an autobiography from John McAfee so badly          0   \n",
       "\n",
       "   char_count_title  word_count_title  char_count_selftext  \\\n",
       "0                46                10                  219   \n",
       "1                42                 8                  285   \n",
       "2                34                 6                  269   \n",
       "3                62                13                  179   \n",
       "4                49                 9                  144   \n",
       "\n",
       "   word_count_selftext                                   title + selftext  \\\n",
       "0                   36  Why is almost all music seem to be about love?...   \n",
       "1                   55  pieces of advice that have stuck with you? I t...   \n",
       "2                   50  Worried about people stuck at home I just can'...   \n",
       "3                   24  How a 5 Dollar bill can help you to feel bette...   \n",
       "4                   28  I want an autobiography from John McAfee so ba...   \n",
       "\n",
       "                            clean_title  \\\n",
       "0                almost music seem love   \n",
       "1                   pieces advice stuck   \n",
       "2             worried people stuck home   \n",
       "3          dollar bill help feel better   \n",
       "4  want autobiography john mcafee badly   \n",
       "\n",
       "                                      clean_selftext  \\\n",
       "0  wow realize much music love romance seems like...   \n",
       "1  thought time would nice hear words encourageme...   \n",
       "2  shake worry kids adults stuck home lock sexual...   \n",
       "3  https youtu awrnmcmza one amazing uplifting vi...   \n",
       "4  imagine insane book would make even better cou...   \n",
       "\n",
       "                              clean_title_+_selftext    neg    neu    pos  \\\n",
       "0  almost music seem love wow realize much music ...  0.080  0.301  0.619   \n",
       "1  pieces advice stuck thought time would nice he...  0.091  0.383  0.526   \n",
       "2  worried people stuck home shake worry kids adu...  0.467  0.456  0.077   \n",
       "3  dollar bill help feel better https youtu awrnm...  0.000  0.630  0.370   \n",
       "4  want autobiography john mcafee badly imagine i...  0.215  0.630  0.156   \n",
       "\n",
       "   compound  \n",
       "0    0.9777  \n",
       "1    0.9657  \n",
       "2   -0.8924  \n",
       "3    0.8555  \n",
       "4   -0.3818  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21825, 20)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                     0\n",
       "id                         0\n",
       "num_comments               0\n",
       "score                      0\n",
       "created_utc                0\n",
       "selftext                   0\n",
       "title                      0\n",
       "subreddit                  0\n",
       "char_count_title           0\n",
       "word_count_title           0\n",
       "char_count_selftext        0\n",
       "word_count_selftext        0\n",
       "title + selftext           0\n",
       "clean_title               73\n",
       "clean_selftext             7\n",
       "clean_title_+_selftext     1\n",
       "neg                        0\n",
       "neu                        0\n",
       "pos                        0\n",
       "compound                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.578923\n",
       "1    0.421077\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.579003\n",
       "1    0.420997\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.578996\n",
       "1    0.421004\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723219923849684"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7342284347986022"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Clean Selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7771783320369616"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7855434982527129"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Clean Title + Selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use the best params from selftext\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.9],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9], 'cvec__max_features': [2000],\n",
       "                         'cvec__min_df': [2], 'cvec__ngram_range': [(1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7850882294158469"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864124103255871"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980503954386611"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = gs.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850389355570544"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984182453558948"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Transform training and testing data based on the fit CountVectorizer.\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684162119075357"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Multinomial Naive Bayes model.\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit model.\n",
    "mnb.fit(X_train_cv, y_train)\n",
    "\n",
    "# Evaluate predictions.\n",
    "mnb.score(X_train_cv, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8199374655140702"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the Clean Title + Selftext differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2),\n",
    "                             min_df = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864124103255871"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "# Fit model to training data.\n",
    "lr.fit(X_train_vectorizer, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980503954386611"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_vectorizer, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title + selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english',\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9321846833036973"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "# Fit model to training data.\n",
    "lr.fit(X_train_vectorizer, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7914290969284532"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_vectorizer, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a model with word count and sentiment analysis included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'id', 'num_comments', 'score', 'created_utc', 'selftext',\n",
       "       'title', 'subreddit', 'char_count_title', 'word_count_title',\n",
       "       'char_count_selftext', 'word_count_selftext', 'title + selftext',\n",
       "       'clean_title', 'clean_selftext', 'clean_title_+_selftext', 'neg', 'neu',\n",
       "       'pos', 'compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit[['word_count_title', 'word_count_selftext', 'clean_title_+_selftext', 'neg', 'neu',\n",
    "       'pos', 'compound']]\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 42, \n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16309, 7), (5437, 7))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>clean_title_+_selftext</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>17</td>\n",
       "      <td>233</td>\n",
       "      <td>person excited spend extra time spouse possibl...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.9941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>yeah due corona work hi guys live sound engine...</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>13</td>\n",
       "      <td>542</td>\n",
       "      <td>people underestimate effect culture idea men w...</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.9854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>19</td>\n",
       "      <td>195</td>\n",
       "      <td>secretly hoping corona virus takes massive tol...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.9153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "      <td>despite media says things revert back pre covi...</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.3191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count_title  word_count_selftext  \\\n",
       "19518                17                  233   \n",
       "21027                 9                  103   \n",
       "8544                 13                  542   \n",
       "426                  19                  195   \n",
       "17529                16                   90   \n",
       "\n",
       "                                  clean_title_+_selftext    neg    neu    pos  \\\n",
       "19518  person excited spend extra time spouse possibl...  0.028  0.592  0.380   \n",
       "21027  yeah due corona work hi guys live sound engine...  0.198  0.658  0.144   \n",
       "8544   people underestimate effect culture idea men w...  0.242  0.623  0.135   \n",
       "426    secretly hoping corona virus takes massive tol...  0.227  0.636  0.136   \n",
       "17529  despite media says things revert back pre covi...  0.219  0.593  0.188   \n",
       "\n",
       "       compound  \n",
       "19518    0.9941  \n",
       "21027   -0.4567  \n",
       "8544    -0.9854  \n",
       "426     -0.9153  \n",
       "17529   -0.3191  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate countvectorize\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2),\n",
    "                             min_df = 2) \n",
    "\n",
    "# transform X_train and X_test\n",
    "X_train_cv = cvec.fit_transform(X_train['clean_title_+_selftext'])\n",
    "X_test_cv = cvec.transform(X_test['clean_title_+_selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to use .todense() to transform your sparse matrix to a matrix sklearn can use in a model\n",
    "X_train_df = pd.DataFrame(X_train_cv.todense(), columns= cvec.get_feature_names(), index=X_train.index)\n",
    "\n",
    "# create X_testing df for model\n",
    "X_test_df = pd.DataFrame(X_test_cv.todense(), columns= cvec.get_feature_names(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able get</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abused</th>\n",
       "      <th>abusive</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>...</th>\n",
       "      <th>years old</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube com</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  able get  absolute  absolutely  abuse  abused  abusive  \\\n",
       "19518        0     2         0         0           0      0       0        0   \n",
       "21027        0     0         0         0           0      0       0        0   \n",
       "8544         0     0         0         0           0      0       0        0   \n",
       "426          0     0         0         0           0      0       0        0   \n",
       "17529        0     0         0         0           0      0       0        0   \n",
       "\n",
       "       accept  accepted  ...  years old  yes  yesterday  yet  yo  young  \\\n",
       "19518       0         0  ...          0    0          0    0   0      0   \n",
       "21027       0         0  ...          0    0          0    0   0      0   \n",
       "8544        0         0  ...          0    0          0    1   0      0   \n",
       "426         0         0  ...          0    0          0    0   0      0   \n",
       "17529       0         0  ...          0    0          0    0   0      0   \n",
       "\n",
       "       younger  youtube  youtube com  zero  \n",
       "19518        0        0            0     0  \n",
       "21027        0        0            0     0  \n",
       "8544         0        0            0     0  \n",
       "426          0        0            0     0  \n",
       "17529        0        0            0     0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able get</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abused</th>\n",
       "      <th>abusive</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube com</th>\n",
       "      <th>zero</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>word_count_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>17</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.4567</td>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.9854</td>\n",
       "      <td>13</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.9153</td>\n",
       "      <td>19</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.3191</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  able get  absolute  absolutely  abuse  abused  abusive  \\\n",
       "19518        0     2         0         0           0      0       0        0   \n",
       "21027        0     0         0         0           0      0       0        0   \n",
       "8544         0     0         0         0           0      0       0        0   \n",
       "426          0     0         0         0           0      0       0        0   \n",
       "17529        0     0         0         0           0      0       0        0   \n",
       "\n",
       "       accept  accepted  ...  younger  youtube  youtube com  zero    neg  \\\n",
       "19518       0         0  ...        0        0            0     0  0.028   \n",
       "21027       0         0  ...        0        0            0     0  0.198   \n",
       "8544        0         0  ...        0        0            0     0  0.242   \n",
       "426         0         0  ...        0        0            0     0  0.227   \n",
       "17529       0         0  ...        0        0            0     0  0.219   \n",
       "\n",
       "         neu    pos  compound  word_count_title  word_count_selftext  \n",
       "19518  0.592  0.380    0.9941                17                  233  \n",
       "21027  0.658  0.144   -0.4567                 9                  103  \n",
       "8544   0.623  0.135   -0.9854                13                  542  \n",
       "426    0.636  0.136   -0.9153                19                  195  \n",
       "17529  0.593  0.188   -0.3191                16                   90  \n",
       "\n",
       "[5 rows x 2006 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = pd.merge(left = X_train_df,\n",
    "                     right = X_train[['neg', \n",
    "                                      'neu', \n",
    "                                      'pos', \n",
    "                                      'compound',\n",
    "                                      'word_count_title', \n",
    "                                      'word_count_selftext']],\n",
    "                     left_index = True,\n",
    "                     right_index = True)\n",
    "\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.merge(left = X_test_df,\n",
    "                    right = X_test[['neg', \n",
    "                                      'neu', \n",
    "                                      'pos', \n",
    "                                      'compound',\n",
    "                                      'word_count_title', \n",
    "                                      'word_count_selftext']],\n",
    "                    left_index = True,\n",
    "                    right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16309, 2006), (16309,))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5437, 2006), (5437,))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate LogisticRegression\n",
    "lr = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "\n",
    "# fit to the training data\n",
    "lr.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.8672512109877981\n",
      "test: 0.8057752437005702\n"
     ]
    }
   ],
   "source": [
    "# check score for train and test\n",
    "print('train:', lr.score(X_train_df, y_train))\n",
    "print('test:', lr.score(X_test_df, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
