{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I run and test as many different models.\n",
    "\n",
    "I was interested in seeing the difference between titles and selftext and the feature engineered column I created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = pd.read_csv('Datasets/reddit_cleaned_title_and_selftext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>char_count_title</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>char_count_selftext</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>title + selftext</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_selftext</th>\n",
       "      <th>clean_title_+_selftext</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothanksbud5</td>\n",
       "      <td>g5rffm</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1587516214</td>\n",
       "      <td>Wow i didn’t realize how much music is about b...</td>\n",
       "      <td>Why is almost all music seem to be about love?</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>219</td>\n",
       "      <td>36</td>\n",
       "      <td>Why is almost all music seem to be about love?...</td>\n",
       "      <td>almost music seem love</td>\n",
       "      <td>wow realize much music love romance seems like...</td>\n",
       "      <td>almost music seem love wow realize much music ...</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dontknowwhattdo</td>\n",
       "      <td>g5r7z2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1587515419</td>\n",
       "      <td>I thought that during this time it would be ni...</td>\n",
       "      <td>pieces of advice that have stuck with you?</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>285</td>\n",
       "      <td>55</td>\n",
       "      <td>pieces of advice that have stuck with you? I t...</td>\n",
       "      <td>pieces advice stuck</td>\n",
       "      <td>thought time would nice hear words encourageme...</td>\n",
       "      <td>pieces advice stuck thought time would nice he...</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sharkfinnsouphk</td>\n",
       "      <td>g5r5q2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1587515173</td>\n",
       "      <td>I just can't shake this worry about kids (and ...</td>\n",
       "      <td>Worried about people stuck at home</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>269</td>\n",
       "      <td>50</td>\n",
       "      <td>Worried about people stuck at home I just can'...</td>\n",
       "      <td>worried people stuck home</td>\n",
       "      <td>shake worry kids adults stuck home lock sexual...</td>\n",
       "      <td>worried people stuck home shake worry kids adu...</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.8924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dehlen1me</td>\n",
       "      <td>g5r3t3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1587514972</td>\n",
       "      <td>https://youtu.be/9_AWrNmcMZA\\nThis is one of t...</td>\n",
       "      <td>How a 5 Dollar bill can help you to feel bette...</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>179</td>\n",
       "      <td>24</td>\n",
       "      <td>How a 5 Dollar bill can help you to feel bette...</td>\n",
       "      <td>dollar bill help feel better</td>\n",
       "      <td>https youtu awrnmcmza one amazing uplifting vi...</td>\n",
       "      <td>dollar bill help feel better https youtu awrnm...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.8555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fighterpilot909</td>\n",
       "      <td>g5qtjo</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1587513886</td>\n",
       "      <td>Imagine how insane that book would be. To make...</td>\n",
       "      <td>I want an autobiography from John McAfee so badly</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "      <td>I want an autobiography from John McAfee so ba...</td>\n",
       "      <td>want autobiography john mcafee badly</td>\n",
       "      <td>imagine insane book would make even better cou...</td>\n",
       "      <td>want autobiography john mcafee badly imagine i...</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author      id  num_comments  score  created_utc  \\\n",
       "0     nothanksbud5  g5rffm             1      2   1587516214   \n",
       "1  dontknowwhattdo  g5r7z2             3      2   1587515419   \n",
       "2  sharkfinnsouphk  g5r5q2             2      0   1587515173   \n",
       "3        dehlen1me  g5r3t3             0      1   1587514972   \n",
       "4  fighterpilot909  g5qtjo             2      2   1587513886   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Wow i didn’t realize how much music is about b...   \n",
       "1  I thought that during this time it would be ni...   \n",
       "2  I just can't shake this worry about kids (and ...   \n",
       "3  https://youtu.be/9_AWrNmcMZA\\nThis is one of t...   \n",
       "4  Imagine how insane that book would be. To make...   \n",
       "\n",
       "                                               title  subreddit  \\\n",
       "0     Why is almost all music seem to be about love?          0   \n",
       "1         pieces of advice that have stuck with you?          0   \n",
       "2                 Worried about people stuck at home          0   \n",
       "3  How a 5 Dollar bill can help you to feel bette...          0   \n",
       "4  I want an autobiography from John McAfee so badly          0   \n",
       "\n",
       "   char_count_title  word_count_title  char_count_selftext  \\\n",
       "0                46                10                  219   \n",
       "1                42                 8                  285   \n",
       "2                34                 6                  269   \n",
       "3                62                13                  179   \n",
       "4                49                 9                  144   \n",
       "\n",
       "   word_count_selftext                                   title + selftext  \\\n",
       "0                   36  Why is almost all music seem to be about love?...   \n",
       "1                   55  pieces of advice that have stuck with you? I t...   \n",
       "2                   50  Worried about people stuck at home I just can'...   \n",
       "3                   24  How a 5 Dollar bill can help you to feel bette...   \n",
       "4                   28  I want an autobiography from John McAfee so ba...   \n",
       "\n",
       "                            clean_title  \\\n",
       "0                almost music seem love   \n",
       "1                   pieces advice stuck   \n",
       "2             worried people stuck home   \n",
       "3          dollar bill help feel better   \n",
       "4  want autobiography john mcafee badly   \n",
       "\n",
       "                                      clean_selftext  \\\n",
       "0  wow realize much music love romance seems like...   \n",
       "1  thought time would nice hear words encourageme...   \n",
       "2  shake worry kids adults stuck home lock sexual...   \n",
       "3  https youtu awrnmcmza one amazing uplifting vi...   \n",
       "4  imagine insane book would make even better cou...   \n",
       "\n",
       "                              clean_title_+_selftext    neg    neu    pos  \\\n",
       "0  almost music seem love wow realize much music ...  0.080  0.301  0.619   \n",
       "1  pieces advice stuck thought time would nice he...  0.091  0.383  0.526   \n",
       "2  worried people stuck home shake worry kids adu...  0.467  0.456  0.077   \n",
       "3  dollar bill help feel better https youtu awrnm...  0.000  0.630  0.370   \n",
       "4  want autobiography john mcafee badly imagine i...  0.215  0.630  0.156   \n",
       "\n",
       "   compound  \n",
       "0    0.9777  \n",
       "1    0.9657  \n",
       "2   -0.8924  \n",
       "3    0.8555  \n",
       "4   -0.3818  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21825, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                    0\n",
       "id                        0\n",
       "num_comments              0\n",
       "score                     0\n",
       "created_utc               0\n",
       "selftext                  0\n",
       "title                     0\n",
       "subreddit                 0\n",
       "char_count_title          0\n",
       "word_count_title          0\n",
       "char_count_selftext       0\n",
       "word_count_selftext       0\n",
       "title + selftext          0\n",
       "clean_title               0\n",
       "clean_selftext            0\n",
       "clean_title_+_selftext    0\n",
       "neg                       0\n",
       "neu                       0\n",
       "pos                       0\n",
       "compound                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.578923\n",
       "1    0.421077\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.579003\n",
       "1    0.420997\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason when I pulled in my csv there was missing values. I checked my previous notebook(03_Further_Cleaning.ipynb), but couldn't find out why there were added null values, but there weren't too many of them so I decided to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the clean titles I created to see how they run in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.578996\n",
       "1    0.421004\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723219923849684"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7342284347986022"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not too surpised to see that the model didn't score too high since there wasn't as much information on the titles as other columns like selftext. It seems to fit people well to the test model as well. Remembering from the EDA that the titles were about the same in word count. So this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Clean Selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the clean selftext I created to see how they run in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2), (2,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7771783320369616"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7855434982527129"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column did perform better then the title column, but was hoping for a better score. Now I would like to see how the feature engineered column did.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Clean Title + Selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on the clean titles and selftext I created to see how they run in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use the best params from selftext\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2000],\n",
    "    'cvec__min_df': [2],\n",
    "    'cvec__max_df': [.9],\n",
    "    'cvec__ngram_range': [(1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9], 'cvec__max_features': [2000],\n",
       "                         'cvec__min_df': [2], 'cvec__ngram_range': [(1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7850882294158469"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864124103255871"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980503954386611"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = gs.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850389355570544"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984182453558948"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though the feature engineered column faired a little better, but not too much. Also after doing a little more to the model I realized it was quite overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684162119075357"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_train_cv, y_train)\n",
    "\n",
    "mnb.score(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8199374655140702"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(X_test_cv, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the last three models this one seems to be performing the best. It is a little overfit, but not too much. I would be interested to see how the score would be if I gathered more subreddit submissions and did a little better cleaning if this model would perform a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out the Clean Title + Selftext differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to see if I ran a model without the pipeline and different hyperparameters how it will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['clean_title_+_selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2),\n",
    "                             min_df = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864124103255871"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "# Fit model to training data.\n",
    "lr.fit(X_train_vectorizer, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980503954386611"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_vectorizer, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the outcomes between the 'clean' title + selftext and the normal title + selftext to see if they were performing any differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title + selftext']\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating CountVectorizer \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english',\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorizer = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_vectorizer = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9321846833036973"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "# Fit model to training data.\n",
    "lr.fit(X_train_vectorizer, y_train)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "lr.score(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7914290969284532"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_vectorizer, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the two models has showed me that the two columns are scoring at very different percentages. At least in the training sets. The test sets are performing pretty similarly. The major difference is that the untampered 'title + selftext' is way more overfit. By about 14% points where the clean column is only overfit by 7%. Which seems to be better, but both testing sets performed about the same. Which makes me think if I need a lot more data. Maybe 21,000 rows isn't enough data to parse though to see a drastic difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a model with word count and sentiment analysis included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was interested to see if the model would perform better if I tried adding in more columns. I was wondering if a sentiment analysis would help the testing set improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'id', 'num_comments', 'score', 'created_utc', 'selftext',\n",
       "       'title', 'subreddit', 'char_count_title', 'word_count_title',\n",
       "       'char_count_selftext', 'word_count_selftext', 'title + selftext',\n",
       "       'clean_title', 'clean_selftext', 'clean_title_+_selftext', 'neg', 'neu',\n",
       "       'pos', 'compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit[['word_count_title', 'word_count_selftext', 'clean_title_+_selftext', 'neg', 'neu',\n",
    "       'pos', 'compound', 'num_comments']]\n",
    "y = df_reddit['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 42, \n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16309, 8), (5437, 8))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>clean_title_+_selftext</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>17</td>\n",
       "      <td>233</td>\n",
       "      <td>person excited spend extra time spouse possibl...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>yeah due corona work hi guys live sound engine...</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.4567</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>13</td>\n",
       "      <td>542</td>\n",
       "      <td>people underestimate effect culture idea men w...</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.9854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>19</td>\n",
       "      <td>195</td>\n",
       "      <td>secretly hoping corona virus takes massive tol...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.9153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "      <td>despite media says things revert back pre covi...</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.3191</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count_title  word_count_selftext  \\\n",
       "19518                17                  233   \n",
       "21027                 9                  103   \n",
       "8544                 13                  542   \n",
       "426                  19                  195   \n",
       "17529                16                   90   \n",
       "\n",
       "                                  clean_title_+_selftext    neg    neu    pos  \\\n",
       "19518  person excited spend extra time spouse possibl...  0.028  0.592  0.380   \n",
       "21027  yeah due corona work hi guys live sound engine...  0.198  0.658  0.144   \n",
       "8544   people underestimate effect culture idea men w...  0.242  0.623  0.135   \n",
       "426    secretly hoping corona virus takes massive tol...  0.227  0.636  0.136   \n",
       "17529  despite media says things revert back pre covi...  0.219  0.593  0.188   \n",
       "\n",
       "       compound  num_comments  \n",
       "19518    0.9941            10  \n",
       "21027   -0.4567             2  \n",
       "8544    -0.9854             0  \n",
       "426     -0.9153             1  \n",
       "17529   -0.3191             5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorizing the clean title + selftext\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 2000,\n",
    "                             ngram_range= (1,2),\n",
    "                             min_df = 2) \n",
    "\n",
    "\n",
    "X_train_cv = cvec.fit_transform(X_train['clean_title_+_selftext'])\n",
    "X_test_cv = cvec.transform(X_test['clean_title_+_selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the CountVectorized data into dataframes\n",
    "X_train_df = pd.DataFrame(X_train_cv.todense(), columns= cvec.get_feature_names(), index=X_train.index)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_cv.todense(), columns= cvec.get_feature_names(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able get</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abused</th>\n",
       "      <th>abusive</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>...</th>\n",
       "      <th>years old</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube com</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  able get  absolute  absolutely  abuse  abused  abusive  \\\n",
       "19518        0     2         0         0           0      0       0        0   \n",
       "21027        0     0         0         0           0      0       0        0   \n",
       "8544         0     0         0         0           0      0       0        0   \n",
       "426          0     0         0         0           0      0       0        0   \n",
       "17529        0     0         0         0           0      0       0        0   \n",
       "\n",
       "       accept  accepted  ...  years old  yes  yesterday  yet  yo  young  \\\n",
       "19518       0         0  ...          0    0          0    0   0      0   \n",
       "21027       0         0  ...          0    0          0    0   0      0   \n",
       "8544        0         0  ...          0    0          0    1   0      0   \n",
       "426         0         0  ...          0    0          0    0   0      0   \n",
       "17529       0         0  ...          0    0          0    0   0      0   \n",
       "\n",
       "       younger  youtube  youtube com  zero  \n",
       "19518        0        0            0     0  \n",
       "21027        0        0            0     0  \n",
       "8544         0        0            0     0  \n",
       "426          0        0            0     0  \n",
       "17529        0        0            0     0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able get</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abused</th>\n",
       "      <th>abusive</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube com</th>\n",
       "      <th>zero</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>17</td>\n",
       "      <td>233</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.144</td>\n",
       "      <td>-0.4567</td>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-0.9854</td>\n",
       "      <td>13</td>\n",
       "      <td>542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-0.9153</td>\n",
       "      <td>19</td>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.3191</td>\n",
       "      <td>16</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2007 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ability  able  able get  absolute  absolutely  abuse  abused  abusive  \\\n",
       "19518        0     2         0         0           0      0       0        0   \n",
       "21027        0     0         0         0           0      0       0        0   \n",
       "8544         0     0         0         0           0      0       0        0   \n",
       "426          0     0         0         0           0      0       0        0   \n",
       "17529        0     0         0         0           0      0       0        0   \n",
       "\n",
       "       accept  accepted  ...  youtube  youtube com  zero    neg    neu    pos  \\\n",
       "19518       0         0  ...        0            0     0  0.028  0.592  0.380   \n",
       "21027       0         0  ...        0            0     0  0.198  0.658  0.144   \n",
       "8544        0         0  ...        0            0     0  0.242  0.623  0.135   \n",
       "426         0         0  ...        0            0     0  0.227  0.636  0.136   \n",
       "17529       0         0  ...        0            0     0  0.219  0.593  0.188   \n",
       "\n",
       "       compound  word_count_title  word_count_selftext  num_comments  \n",
       "19518    0.9941                17                  233            10  \n",
       "21027   -0.4567                 9                  103             2  \n",
       "8544    -0.9854                13                  542             0  \n",
       "426     -0.9153                19                  195             1  \n",
       "17529   -0.3191                16                   90             5  \n",
       "\n",
       "[5 rows x 2007 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 'neg', 'neu', 'pos', 'compound', 'word_count_title', 'word_count_selftext', \n",
    "#'num_comments' back to training set\n",
    "X_train_df = pd.merge(left = X_train_df,\n",
    "                     right = X_train[['neg', \n",
    "                                      'neu', \n",
    "                                      'pos', \n",
    "                                      'compound',\n",
    "                                      'word_count_title', \n",
    "                                      'word_count_selftext',\n",
    "                                      'num_comments']],\n",
    "                     left_index = True,\n",
    "                     right_index = True)\n",
    "\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'neg', 'neu', 'pos', 'compound', 'word_count_title', 'word_count_selftext', \n",
    "#'num_comments' back to testing set\n",
    "X_test_df = pd.merge(left = X_test_df,\n",
    "                    right = X_test[['neg', \n",
    "                                      'neu', \n",
    "                                      'pos', \n",
    "                                      'compound',\n",
    "                                      'word_count_title', \n",
    "                                      'word_count_selftext',\n",
    "                                      'num_comments']],\n",
    "                    left_index = True,\n",
    "                    right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16309, 2007), (16309,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5437, 2007), (5437,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear', penalty='l2')\n",
    "\n",
    "lr.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.8669446317983935\n",
      "test: 0.8033842192385506\n"
     ]
    }
   ],
   "source": [
    "print('train:', lr.score(X_train_df, y_train))\n",
    "print('test:', lr.score(X_test_df, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing set was a lot higher than some of the other models I ran. I still think that the Naive Bayes model did the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to find out what words were most helpful in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lr.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21592967, -0.11841878, -0.57113855, ...,  0.01556339,\n",
       "         0.00526231, -0.00167541]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(results.coef_,\n",
    "             columns=X_train_df.columns).T.sort_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quarantine</th>\n",
       "      <td>-2.026719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poll https</th>\n",
       "      <td>-1.961879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>view poll</th>\n",
       "      <td>-1.924721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted share</th>\n",
       "      <td>-1.709266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corona</th>\n",
       "      <td>-1.599827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexually</th>\n",
       "      <td>1.686206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>1.853095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>1.962149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <td>2.197139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religious</th>\n",
       "      <td>2.371477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2007 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "quarantine   -2.026719\n",
       "poll https   -1.961879\n",
       "view poll    -1.924721\n",
       "wanted share -1.709266\n",
       "corona       -1.599827\n",
       "...                ...\n",
       "sexually      1.686206\n",
       "died          1.853095\n",
       "death         1.962149\n",
       "sexual        2.197139\n",
       "religious     2.371477\n",
       "\n",
       "[2007 rows x 1 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping words I don't think should be there\n",
    "words_df.drop(index=['seriousconversation', 'neg'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>religious</th>\n",
       "      <td>2.371477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <td>2.197139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>1.962149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>died</th>\n",
       "      <td>1.853095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexually</th>\n",
       "      <td>1.686206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>1.667395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trauma</th>\n",
       "      <td>1.660829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancer</th>\n",
       "      <td>1.633291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugs</th>\n",
       "      <td>1.562543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dying</th>\n",
       "      <td>1.441792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>1.379804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>1.366498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dead</th>\n",
       "      <td>1.270756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>1.240917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discuss</th>\n",
       "      <td>1.192002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much time</th>\n",
       "      <td>1.135842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>1.121131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illness</th>\n",
       "      <td>1.077652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passed away</th>\n",
       "      <td>1.058376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porn</th>\n",
       "      <td>1.032302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "religious    2.371477\n",
       "sexual       2.197139\n",
       "death        1.962149\n",
       "died         1.853095\n",
       "sexually     1.686206\n",
       "politics     1.667395\n",
       "trauma       1.660829\n",
       "cancer       1.633291\n",
       "drugs        1.562543\n",
       "dying        1.441792\n",
       "political    1.379804\n",
       "sex          1.366498\n",
       "dead         1.270756\n",
       "religion     1.240917\n",
       "discuss      1.192002\n",
       "much time    1.135842\n",
       "climate      1.121131\n",
       "illness      1.077652\n",
       "passed away  1.058376\n",
       "porn         1.032302"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out what are the top 20 words\n",
    "words_df.loc[(words_df[0] > 1.03), :].sort_values(0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing more words that I feel aren't too useful\n",
    "words_df.drop(index=['poll https', 'view poll', 'neu', 'mid', 'casualconversation wiki'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quarantine</th>\n",
       "      <td>-2.026719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted share</th>\n",
       "      <td>-1.709266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corona</th>\n",
       "      <td>-1.599827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolation</th>\n",
       "      <td>-1.434727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lockdown</th>\n",
       "      <td>-1.287376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caring</th>\n",
       "      <td>-1.267222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cake</th>\n",
       "      <td>-1.224584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay home</th>\n",
       "      <td>-1.188175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social distancing</th>\n",
       "      <td>-1.171571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bought</th>\n",
       "      <td>-1.133384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>-1.109965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applied</th>\n",
       "      <td>-1.078960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tldr</th>\n",
       "      <td>-1.061328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series</th>\n",
       "      <td>-1.052097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>working home</th>\n",
       "      <td>-1.051667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatting</th>\n",
       "      <td>-1.022924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>-0.991696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <td>-0.977520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>holding</th>\n",
       "      <td>-0.971848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>-0.961733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thankful</th>\n",
       "      <td>-0.950105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "quarantine        -2.026719\n",
       "wanted share      -1.709266\n",
       "corona            -1.599827\n",
       "isolation         -1.434727\n",
       "lockdown          -1.287376\n",
       "caring            -1.267222\n",
       "cake              -1.224584\n",
       "stay home         -1.188175\n",
       "social distancing -1.171571\n",
       "bought            -1.133384\n",
       "haha              -1.109965\n",
       "applied           -1.078960\n",
       "tldr              -1.061328\n",
       "series            -1.052097\n",
       "working home      -1.051667\n",
       "chatting          -1.022924\n",
       "coronavirus       -0.991696\n",
       "virus             -0.977520\n",
       "holding           -0.971848\n",
       "covid             -0.961733\n",
       "thankful          -0.950105"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.loc[(words_df[0] < -.95), :].sort_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through the modeling process I realized that there is still some more good work that can be done. Listing out the coefficients I see that there are some other words I could have removed to maybe help the model. Cleaning and parsing out the self text with website links. Removing the subreddit name from all the values. \n",
    "\n",
    "I also think I may need a lot more data. My initial thought of about 22,000 was clearly not enough. I also needed to tweak the hyperparameters more. \n",
    "\n",
    "This dataset was definitly large and not doing proper cleaning I can see how it is pretty exhaustive to your computer. One of the models I ran took 30 minutes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
