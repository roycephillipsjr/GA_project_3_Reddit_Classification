{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://api.pushshift.io/reddit/search/submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_casual_conversation = requests.get(\n",
    "#     base_url, \n",
    "#     params={\n",
    "#         'subreddit': 'CasualConversation', \n",
    "#         'size': 500,\n",
    "#     })\n",
    "\n",
    "# res_serious_conversation = requests.get(\n",
    "#     base_url, \n",
    "#     params={\n",
    "#         'subreddit': 'SeriousConversation', \n",
    "#         'size': 500,\n",
    "#     })\n",
    "\n",
    "# data_casual_conversation = res_casual_conversation.json()['data']\n",
    "# data_serious_conversation = res_serious_conversation.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_casual_conversation[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_serious_conversation[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This code has been modified from Tim Book's Office Hours\n",
    "# def get_submissions(subreddit, n_iter):\n",
    "#     df_list = []\n",
    "#     current_time = 1587431400\n",
    "#     for _ in range(n_iter):\n",
    "#         res = requests.get(\n",
    "#         base_url, \n",
    "#         params={\n",
    "#             'subreddit': subreddit, \n",
    "#             'size': 500,\n",
    "#             'before': current_time\n",
    "#             }\n",
    "#         )\n",
    "#         df = pd.DataFrame(res.json()['data'])\n",
    "#         df = df.loc[:, ['created_utc', 'selftext', 'title', 'subreddit']]\n",
    "#         df_list.append(df)\n",
    "#         current_time = df.created_utc.min()\n",
    "        \n",
    "#     return pd.concat(df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_casual_conversation = get_submissions('CasualConversation', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_casual_conversation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_serious_conversation = get_submissions('SeriousConversation', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_serious_conversation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df = pd.concat([df_casual_conversation, df_serious_conversation], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a very basic model to just get a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Checking for nulls\n",
    "# big_reddit_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing nulls\n",
    "# big_reddit_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# big_reddit_df.loc[big_reddit_df['selftext'] == '[removed]', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the selftext that has been [removed]\n",
    "# big_reddit_df = big_reddit_df[big_reddit_df['selftext'] != '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh my the balance is now way more off! Got to get more CasualConversation\n",
    "# big_reddit_df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Changing CasualConversation = 0 & SeriousConversation = 1\n",
    "# big_reddit_df['subreddit'] = big_reddit_df['subreddit'].map({'CasualConversation': 0, 'SeriousConversation': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# big_reddit_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Character count\n",
    "# big_reddit_df['char_count_title'] = big_reddit_df['title'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df['word_count_title'] = big_reddit_df['title'].map(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df.groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df['char_count_selftext'] = big_reddit_df['selftext'].map(len)\n",
    "# big_reddit_df['word_count_selftext'] = big_reddit_df['selftext'].map(lambda x:len(x.split()))\n",
    "# big_reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# big_reddit_df.groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.hist([big_reddit_df[big_reddit_df['subreddit'] == 0]['char_count_title'],\n",
    "#           big_reddit_df[big_reddit_df['subreddit'] == 1]['char_count_title']],\n",
    "#         bins=20, color=['blue', 'goldenrod'], ec='k')\n",
    "# plt.title('Character Count by Subreddit Title', fontsize=25)\n",
    "# plt.legend(['CasualConversation', 'SeriousConversation']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.hist([big_reddit_df[big_reddit_df['subreddit'] == 0]['word_count_title'],\n",
    "#           big_reddit_df[big_reddit_df['subreddit'] == 1]['word_count_title']],\n",
    "#         bins=20, color=['blue', 'goldenrod'], ec='k')\n",
    "# plt.title('Word Count by Subreddit Title', fontsize=25)\n",
    "# plt.legend(['CasualConversation', 'SeriousConversation']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.hist([big_reddit_df[big_reddit_df['subreddit'] == 0]['char_count_selftext'],\n",
    "#           big_reddit_df[big_reddit_df['subreddit'] == 1]['char_count_selftext']],\n",
    "#         bins=15, color=['blue', 'goldenrod'], ec='k')\n",
    "# plt.title('Character Count by Subreddit Selftext', fontsize=25)\n",
    "# plt.legend(['CasualConversation', 'SeriousConversation']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.hist([big_reddit_df[big_reddit_df['subreddit'] == 0]['word_count_selftext'],\n",
    "#           big_reddit_df[big_reddit_df['subreddit'] == 1]['word_count_selftext']],\n",
    "#         bins=15, color=['blue', 'goldenrod'], ec='k')\n",
    "# plt.title('Word Count by Subreddit Selftext', fontsize=25)\n",
    "# plt.legend(['CasualConversation', 'SeriousConversation']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_reddit_df['title + selftext'] = big_reddit_df['title'] + ' ' + big_reddit_df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = big_reddit_df[['selftext', 'title']]\n",
    "# X = big_reddit_df['title']\n",
    "# y = big_reddit_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                     y,\n",
    "#                                                     test_size=0.25,\n",
    "#                                                     stratify=y,\n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('cvec', CountVectorizer()),\n",
    "#     ('lr', LogisticRegression(max_iter=5000))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_params = {\n",
    "#     'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "#     'cvec__min_df': [1, 2, 3 , 4],\n",
    "#     'cvec__max_df': [.9, .95],\n",
    "#     'cvec__ngram_range': [(1,1), (1,2), (1,3)]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = GridSearchCV(pipe,\n",
    "#                   pipe_params,\n",
    "#                   cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7205104565637392"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', min_df=5, max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = cvec.fit_transform(big_reddit_df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1932"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now testing to see how it works with the title and self text combined\n",
    "X = big_reddit_df['title + selftext']\n",
    "y = big_reddit_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2000, 3000, 4000, 5000],\n",
    "    'cvec__min_df': [1, 2, 3],\n",
    "    'cvec__max_df': [.9],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__max_iter': [5000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe,\n",
    "                  pipe_params,\n",
    "                  cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.9],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [1, 2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'lr__max_iter': [5000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7942797530947888"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__max_features': 5000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'lr__max_iter': 5000}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7952871870397643"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.9, max_features=5000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.best_estimator_.steps[1][1].coef_,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
